<!DOCTYPE html><html lang="en" data-astro-cid-bvzihdzo> <head><!-- Global Metadata --><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><meta name="generator" content="Astro v5.1.1"><!-- Font preloads --><link rel="preload" href="/fonts/atkinson-regular.woff" as="font" type="font/woff" crossorigin><link rel="preload" href="/fonts/atkinson-bold.woff" as="font" type="font/woff" crossorigin><!-- Canonical URL --><link rel="canonical" href="https://gowind.github.io/blog/thinkingincuda/"><!-- Primary Meta Tags --><title>Thinking in CUDA (or what I learnt in April 2025)</title><meta name="title" content="Thinking in CUDA (or what I learnt in April 2025)"><meta name="description" content="Learning CUDA and getting into the mindset of a GPU"><!-- Open Graph / Facebook --><meta property="og:type" content="website"><meta property="og:url" content="https://gowind.github.io/blog/thinkingincuda/"><meta property="og:title" content="Thinking in CUDA (or what I learnt in April 2025)"><meta property="og:description" content="Learning CUDA and getting into the mindset of a GPU"><meta property="og:image" content="https://gowind.github.io/blog-placeholder-1.jpg"><!-- Twitter --><meta property="twitter:card" content="summary_large_image"><meta property="twitter:url" content="https://gowind.github.io/blog/thinkingincuda/"><meta property="twitter:title" content="Thinking in CUDA (or what I learnt in April 2025)"><meta property="twitter:description" content="Learning CUDA and getting into the mindset of a GPU"><meta property="twitter:image" content="https://gowind.github.io/blog-placeholder-1.jpg"><!-- Cloudflare Web Analytics --><script defer src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon="{&quot;token&quot;: &quot;4de85a62d838459fafd8a042c2f1c7c2&quot;}"></script><!-- End Cloudflare Web Analytics --><style>:root{--accent: #2337ff;--accent-dark: #000d8a;--black: 15, 18, 25;--gray: 96, 115, 159;--gray-light: 229, 233, 240;--gray-dark: 34, 41, 57;--gray-gradient: rgba(var(--gray-light), 50%), #fff;--box-shadow: 0 2px 6px rgba(var(--gray), 25%), 0 8px 24px rgba(var(--gray), 33%), 0 16px 32px rgba(var(--gray), 33%)}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-regular.woff) format("woff");font-weight:400;font-style:normal;font-display:swap}@font-face{font-family:Atkinson;src:url(/fonts/atkinson-bold.woff) format("woff");font-weight:700;font-style:normal;font-display:swap}body{font-family:Atkinson,sans-serif;margin:0;padding:0;text-align:left;background:linear-gradient(var(--gray-gradient)) no-repeat;background-size:100% 600px;word-wrap:break-word;overflow-wrap:break-word;color:rgb(var(--gray-dark));font-size:20px;line-height:1.7}main{width:720px;max-width:calc(100% - 2em);margin:auto;padding:3em 1em}h1,h2,h3,h4,h5,h6{margin:0 0 .5rem;color:rgb(var(--black));line-height:1.2}h1{font-size:3.052em}h2{font-size:2.441em}h3{font-size:1.953em}h4{font-size:1.563em}h5{font-size:1.25em}strong,b{font-weight:700}a,a:hover{color:var(--accent)}p{margin-bottom:1em}.prose p{margin-bottom:2em}textarea{width:100%;font-size:16px}input{font-size:16px}table{width:100%}img{max-width:100%;height:auto;border-radius:8px}code{padding:2px 5px;background-color:rgb(var(--gray-light));border-radius:2px}pre{padding:1.5em;border-radius:8px}pre>code{all:unset}blockquote{border-left:4px solid var(--accent);padding:0 0 0 20px;margin:0;font-size:1.333em}hr{border:none;border-top:1px solid rgb(var(--gray-light))}@media (max-width: 720px){body{font-size:18px}main{padding:1em}}.sr-only{border:0;padding:0;margin:0;position:absolute!important;height:1px;width:1px;overflow:hidden;clip:rect(1px 1px 1px 1px);clip:rect(1px,1px,1px,1px);clip-path:inset(50%);white-space:nowrap}.tag{display:inline-block;background-color:#f3f4f6;border-radius:.5rem;padding:.25rem .75rem;margin:0 .5rem .5rem 0;font-size:.875rem;font-weight:600;color:#4b5563}header[data-astro-cid-3ef6ksr2]{margin:0;padding:0 1em;background:#fff;box-shadow:0 2px 8px rgba(var(--black),5%)}h2[data-astro-cid-3ef6ksr2]{margin:0;font-size:1em}nav[data-astro-cid-3ef6ksr2]{display:flex;align-items:center;justify-content:space-between}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{padding:1em .5em;color:var(--black);border-bottom:4px solid transparent;text-decoration:none}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]:hover{color:var(--accent);color:#a12323;animation:shake .2s;animation-iteration-count:1;text-shadow:2px 4px rgba(0,0,0,.2);font-weight:bolder}@keyframes shake{0%{transform:translate(1px,1px) rotate(0)}10%{transform:translate(-1px,-2px) rotate(-5deg)}20%{transform:translate(-3px) rotate(5deg)}30%{transform:translate(3px,2px) rotate(0)}40%{transform:translate(1px,-1px) rotate(5deg)}50%{transform:translate(-1px,2px) rotate(-5deg)}60%{transform:translate(-3px,1px) rotate(0)}70%{transform:translate(3px,1px) rotate(-5deg)}80%{transform:translate(-1px,-1px) rotate(5deg)}90%{transform:translate(1px,2px) rotate(0)}to{transform:translate(1px,-2px) rotate(-5deg)}}nav[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2].active{text-decoration:none;border-bottom-color:#a12323}.social-links[data-astro-cid-3ef6ksr2],.social-links[data-astro-cid-3ef6ksr2] a[data-astro-cid-3ef6ksr2]{display:flex}@media (max-width: 720px){.social-links[data-astro-cid-3ef6ksr2]{display:none}}footer[data-astro-cid-sz7xmlte]{padding:2em 1em 6em;background:linear-gradient(var(--gray-gradient)) no-repeat;color:rgb(var(--gray));text-align:center}.social-links[data-astro-cid-sz7xmlte]{display:flex;justify-content:center;gap:1em;margin-top:1em}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]{text-decoration:none;color:rgb(var(--gray))}.social-links[data-astro-cid-sz7xmlte] a[data-astro-cid-sz7xmlte]:hover{color:rgb(var(--gray-dark))}
a[data-astro-cid-eimmu3lg]{display:inline-block;text-decoration:none}a[data-astro-cid-eimmu3lg].active{font-weight:bolder;text-decoration:underline}
.scrollrectangle[data-astro-cid-bvzihdzo]{position:fixed;right:20px;width:20px;height:100px;background-color:#2c65d7;border-radius:10%;transition:top .3s ease-out;transform-origin:center center;animation:squeeze 1s infinite}@keyframes squeeze{0%{transform:scaleY(1)}50%{transform:scaleY(1.5)}to{transform:scaleY(1)}}main[data-astro-cid-bvzihdzo]{width:calc(100% - 2em);max-width:100%;margin:0}.hero-image[data-astro-cid-bvzihdzo]{width:100%}.hero-image[data-astro-cid-bvzihdzo] img[data-astro-cid-bvzihdzo]{display:block;margin:0 auto;border-radius:12px;box-shadow:var(--box-shadow)}.prose[data-astro-cid-bvzihdzo]{width:720px;max-width:calc(100% - 2em);margin:auto;padding:1em;color:rgb(var(--gray-dark))}.title[data-astro-cid-bvzihdzo]{margin-bottom:1em;padding:1em 0;text-align:center;line-height:1}.title[data-astro-cid-bvzihdzo] h1[data-astro-cid-bvzihdzo]{margin:0 0 .5em}.date[data-astro-cid-bvzihdzo]{margin-bottom:.5em;color:rgb(var(--gray))}.last-updated-on[data-astro-cid-bvzihdzo]{font-style:italic}
[data-astro-image]{width:100%;height:auto;object-fit:var(--fit);object-position:var(--pos);aspect-ratio:var(--w) / var(--h)}[data-astro-image=responsive]{max-width:calc(var(--w) * 1px);max-height:calc(var(--h) * 1px)}[data-astro-image=fixed]{width:calc(var(--w) * 1px);height:calc(var(--h) * 1px)}
</style></head> <body data-astro-cid-bvzihdzo> <header data-astro-cid-3ef6ksr2> <nav data-astro-cid-3ef6ksr2> <h2 data-astro-cid-3ef6ksr2><a href="/" data-astro-cid-3ef6ksr2>Govind&#39;s Blog</a></h2> <div class="internal-links" data-astro-cid-3ef6ksr2> <a href="/" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Home </a>  <a href="/blog" class="active" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Blog </a>  <a href="/ideas" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Steal My Ideas </a>  <a href="/editor" data-astro-cid-3ef6ksr2="true" data-astro-cid-eimmu3lg> Image Editor </a>  </div> <div class="social-links" data-astro-cid-3ef6ksr2> <a href="https://x.com/DeepknowledgeU" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>Follow Astro on X</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" astro-icon="social/twitter" data-astro-cid-3ef6ksr2><path fill="currentColor" d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334 0-.14 0-.282-.006-.422A6.685 6.685 0 0 0 16 3.542a6.658 6.658 0 0 1-1.889.518 3.301 3.301 0 0 0 1.447-1.817 6.533 6.533 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.325 9.325 0 0 1-6.767-3.429 3.289 3.289 0 0 0 1.018 4.382A3.323 3.323 0 0 1 .64 6.575v.045a3.288 3.288 0 0 0 2.632 3.218 3.203 3.203 0 0 1-.865.115 3.23 3.23 0 0 1-.614-.057 3.283 3.283 0 0 0 3.067 2.277A6.588 6.588 0 0 1 .78 13.58a6.32 6.32 0 0 1-.78-.045A9.344 9.344 0 0 0 5.026 15z" data-astro-cid-3ef6ksr2></path></svg> </a> <a href="https://github.com/Gowind" target="_blank" data-astro-cid-3ef6ksr2> <span class="sr-only" data-astro-cid-3ef6ksr2>My Github</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" astro-icon="social/github" data-astro-cid-3ef6ksr2><path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-astro-cid-3ef6ksr2></path></svg> </a> </div> </nav> </header>  <main data-astro-cid-bvzihdzo> <article data-astro-cid-bvzihdzo> <div class="scrollrectangle" data-astro-cid-bvzihdzo></div> <script type="module">const o=document.querySelector(".ball"),c=document.querySelector("header"),r=document.documentElement.scrollHeight-window.innerHeight,t=c?.offsetHeight;o.style.top=`${t+100}px`;window.addEventListener("scroll",()=>{const n=window.scrollY/r,l=window.innerHeight-100;let e=n*l;e=e>=t+100?e:t+100,o.style.top=`${e}px`});</script> <div class="hero-image" data-astro-cid-bvzihdzo>  </div> <div class="prose" data-astro-cid-bvzihdzo> <div class="title" data-astro-cid-bvzihdzo> <div class="date" data-astro-cid-bvzihdzo> <time datetime="2025-05-03T22:00:00.000Z"> May 4, 2025 </time>  </div> <h1 data-astro-cid-bvzihdzo>Thinking in CUDA (or what I learnt in April 2025)</h1> <hr data-astro-cid-bvzihdzo> </div>  <h1 id="cuda">CUDA</h1>
<p>I finally fulfilled a decade long dream of learning to do GPU compute by finally setting aside time to learn CUDA. Why didn’t I do it earlier when it had been a life long ambition ? I gave myself many excuses: tried my hand at building web apps, APIs, DevOps. The ZIRP era afforded a lot of opportunities to make money and ML was not something I was interested in the pre-LLM era. Spending a lot of time doing different things gave me the clarity on what really motived me towards computers in the first place: tinkering with hardware and low level primitives and figuring out how to build make them go brrr</p>
<h2 id="so-how-do-you-program-a-gpu">So how do you program a GPU ?</h2>
<p>You would have heard of the word <code>kernel</code> thrown around a lot in the context of GPUs. Kernels are a fancy way of saying <code>functions</code> that are executed on the GPU.</p>
<p>Before kernels, there were shaders. A shader is a function (like a kernel), written in a shading language like <code>GLSL</code> or <code>HLSL</code>. These shaders were used to program the hardware in a GPU to compute vertexes of shapes (mostly triangles) and the colour of output pixels on a screen. The shaders that computed the vertexes and the colour of output pixels are called vertex shaders and fragment shaders respectively.</p>
<p>The syntax of a shading language is similar to C</p>
<pre class="astro-code github-dark" style="background-color:#24292e;color:#e1e4e8; overflow-x: auto;" tabindex="0" data-language="plaintext"><code><span class="line"><span>attribute vec4 vertexPosition;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>uniform mat4 modelMatrix;</span></span>
<span class="line"><span>uniform mat4 viewMatrix;</span></span>
<span class="line"><span>uniform mat4 projectionMatrix;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>void main() {</span></span>
<span class="line"><span>  gl_Position = projectionMatrix * viewMatrix * modelMatrix * vertexPosition;</span></span>
<span class="line"><span>}</span></span></code></pre>
<p>Before the CUDA era, Graphics cards used a fixed-function pipeline architecture. These GPUs were designed with dedicated, hardwired circuits for specific graphics operations, rather than being programmable. The rendering pipeline followed a fixed sequence of operations that couldn’t be modified by developers:</p>
<ol>
<li>Vertex processing (transformations, lighting)</li>
<li>Rasterization (converting vertices to fragments/pixels)</li>
<li>Fragment processing (texturing, coloring)</li>
<li>Output operations (blending, z-buffering)</li>
</ol>
<p>With the Nvidia 8800GTX, NVIDIA introduced <code>Unified Shaders</code>, hardware that could carry out any of the operations supported by the GPU.
Unified Shaders meant that the GPU could be used for general purpose computing as well as graphics processing.</p>
<p>CUDA is a software ecosystem that allowed programmers to write <code>general purpose</code> kernels that could be executed on the GPU. The <code>general purpose</code> meant pretty much anything : image processing, video processing, 3d rendering, scientific computing and the most important one of all today : machine learning. There are even usecases where <a href="https://dl.acm.org/doi/10.1145/3293883.3295706">GPUs were used to accelerate queries on databases using a GPU implementation of a B-Tree</a></p>
<h2 id="thinking-in-terms-of-ops-per-byte">Thinking in terms of Ops per Byte</h2>
<p>The RTX 4090 has GDDR6X memory that can roughly support 1TB/s of bandwidth (roughly it can transfer 1TB/s of data in and out of the GPU).
The RTX 4090 has a peak FP32 performance of 82 Teraflops.</p>
<p>To add 2 FP32s numbers and store the result, we need to load 2 4byte FP32 values and store 1 4byte FP32 value.
This translates to around 12 bytes of memory bandwidth per operation.
To support 82 Teraflops, we need 82 Teraflops * 12 = 984 TB/S of bandwidth which is ~1000x what our memory is capable of supporting !</p>
<p>Even with HBM3, which is capable of 3-5 TB/s of second, we still get only 246-410 TB/s which is still not sufficient to feed our GPU with data fast enough to sustain peak performance.</p>
<p>Therefore when coming up with algorithms to run on the GPU, it makes sense to think in terms of Flops/Byte, that is,the number of operations our kernels perform for every byte of memory that needs to be loaded or stored from/to the global memory. The fewer the loads/stores, the faster the code can run.</p>
<p>Each Streaming Multiprocessor (CPU core of sorts) has a certain amount of shared memory (think of it as the L1/L2 cache equivalent) that can be used to share memory between threads in a block. Fast kernels try to load reused data elements from the global memory into the shared memory and or the registers of each thread to amortize the cost of loading the data from the global memory.</p>
<h2 id="thinking-in-terms-of-output-threads">Thinking in terms of output Threads</h2>
<p>For programmers who have worked with multi-threading and multiprocessing on CPUs, launching 1024 threads or more feels like a lot. The average CPU on a laptop has anywhere between 4 to 12 cores and while threads aren’t as expensive as they used to be modern Operating System, they are still expensive enough and launching more than 2 threads per CPU core doesn’t add a lot of benefits.</p>
<p>In contrast GPUs are designed to handles 1000s of threads and the more work you can do per thread, the more you can get out of the GPU.</p>
<p>In the typical CPU matmul algorithm, you would have one thread that loads the matrix A, the matrix B, computes the elements of the Matrix C and writes the output.</p>
<p>On the GPU however, we launch threads per output index <code>[i][j]</code> and each threads will load and multiply elements of row <code>i</code> of matrix A and column <code>j</code> of matrix B.</p>
<p>Threads in a GPU are grouped into 1/2/3 dimensional blocks and blocks are grouped into a 1/2/3D grid. If we can load elements from A and B that are shared by all the threads in a block, then we amortize the cost of loading the elements from the GPU memory.</p>
<p>Tiled matrix multiplication does exactly this. It splits the length of the rows M into phase, and in each phase loads loads BLOCK_DIMxBLOCK_DIM elements of A and B. Each phase computes a partial sum of the output matrix C and by the time all the phases are complete, the output matrix C is computed. By loading only BLOCK_DIMxBLOCK_DIM elements of A and B in each phase, we make maximum use of the shared memory of each Thread Block and accelerated the matrix multiplication.</p>
<p>When thinking about tiled matrix multiplication, I tried to visualize the problem from the perspective of the output threads. If I am a thread <code>C[i][j]</code> in a block with <code>BLOCK_DIM</code> threads, how do I split the problem into phases ? And what elements of A and B do I load in each block ? In a CPU world, I would go from A and B -> C whereas this time, it helps to think of the problem as going from C -> A &#x26; B. GPU oriented programming needs a shift in perspective in-order to get good at it.</p>
<p><img src="/public/tiled_matmul.jpg" alt="Tiled Matrix Multiplication"></p>
<h2 id="how-am-i-learning-cuda">How am I learning CUDA ?</h2>
<h3 id="practice">Practice</h3>
<p>I use <a href="https://tensara.org/">Tensara</a> to practice writing CUDA kernels. Tensara is kinda like Leetcode, but for ML focused problems that are run on actual GPUs. There is also <a href="https://leetgpu.com/">LeetGPU</a> which has a smaller number of problem at the moment, but the Pro plan seems to offer better features, such as comparisons with other people’s solutions and getting better visibility into the performance of your kernels.
One Tensara, when my code fails the tests(which it often does since Iam  a total noob), it shows only a sample of the input indices that failed to match the expected output, which makes it hard to debug why it fails. Compilation errors however, show up with the exact line number and the location where the platform found an issue when compiling your code. I guess LeetGPU might be different/better (especially on the Pro plan, but I haven’t tried it out yet).</p>
<p>Both platforms seem to be sourcing the problem statments from <a href="https://github.com/ScalingIntelligence/KernelBench">KernelBench</a> which is a collection of ML/GPU focused problems that for LLMs to try and solve, ranging from simple vector addition kernels to full blown architectures.
It is definitely worth trying to ace all the problems in KernelBench to get really good at GPU programming and LLM architectures</p>
<h4 id="honourable-mention-hipscript">Honourable mention: HipScript</h4>
<p><a href="https://hipscript.lights0123.com/">HipScript</a> is an absolutely cracked project written by the Developer Ben Schattinger to compile CUDA kernels to run them on the browser. It uses AMD’s <a href="https://rocm.docs.amd.com/projects/HIP/en/docs-develop/what_is_hip.html">HIP</a> to translate CUDA code into code that can be run on CPUs or NVIDIA and even AMD GPUs ! The <a href="https://lights0123.com/blog/2025/01/07/hip-script/">writeup</a> on how it works is super awesome and I cannot describe in words the sheer respect I have for the developer’s ability and contribution !</p>
<h3 id="resources">Resources</h3>
<p>The gold standard is still the Orange Book : <a href="https://www.amazon.com/Programming-Massively-Parallel-Processors-Hands/dp/0323912311/ref=sr_1_1?crid=FEHXT0PHLO4C&#x26;dib=eyJ2IjoiMSJ9.R2OXts15yPOOtASiENDYxWt_WAnmR-1AkFEvkCcmHmJIEbYxRXSYQR16aUcXKD_d4vsA_ec-EWJL3R0PU0zxxA.NsE2y6u2lSqf7e9_iLnDEpZOlUZoBygbE31JmOeMjLE&#x26;dib_tag=se&#x26;keywords=Programming+Massively+Parallel+Processors%3A+A+Hands-On+Approach&#x26;qid=1746393170&#x26;sprefix=%2Caps%2C153&#x26;sr=8-1">Programming Massively Parallel Processors</a>
The latest edition seems to relatively new (2020) and it teaches all the basic parallel programming patterns and each chapter teach a new technique (Tiled Mat Mul, Convolutions, Parallel Reduction etc) while showing how to make effective use of the hardware to improve performance.
One complaint I have with the book though is that it leaves a small but crucial part of running the algorithms: namely that of calculating the input and the output grid and block sizes. When splitting up an input array into blocks with X number of threads per block, you need to do some calcuations on the size of the grid and the size of each block. This ensures that you launch enough threads to cover each element in the output while optimzing for filing up the shared memory.
Calculating these values by yourself is a great exercise, but when you are in the middle of understanding kernel code and when you are struggling to calculating the thread index values in cases like <code>OUT_DIM * blockIdx.x + threadIdx.x - FILTER_RADIUS</code>, it can be a bit frustrating initially. I am trying this as an opportunity to sharpen my algorithm skills rather than complaining about the book however and help is online aplenty.</p>
<p>I hope someday this book is euologized like the Dragon Book for Compilers or TAOCP for algorithms.</p>
<p>Another book I have read (not fully, but a couple of chapters) is the <a href="https://www.amazon.com/Professional-CUDA-Programming-John-Cheng/dp/1118739329/ref=sr_1_3?crid=2QSNVS6TRRUJR&#x26;dib=eyJ2IjoiMSJ9.xPYkICLuFpzyfyhBcVUEZVE6L6-2EYnjZSvb-FnnKIxThUA2m3z7Nf_fv4bp8PAo7jxXWYdoG6oOO1Ge4_zdLlZZTNKGWrDCrZK7TfJdK_43_iGiK8dCKTHeh_LNG6mcVQtqZVDocBWQuHa-zT7GhJXdsaxgQWsEUcXzkQSsBMACnhZ11WA6SOhnCVk5Rmxgd6ulnc5ZM4kRiuPNevda1d08tE1bOgkjH73S_dzB0Xk.kkXRMeMMZG8CeTM3vpBcaJHu8pjCmToKeYHzsXvX1R8&#x26;dib_tag=se&#x26;keywords=CUDA+C+programming&#x26;qid=1746393605&#x26;sprefix=cuda+c+programmin%2Caps%2C187&#x26;sr=8-3">Professional CUDA C Programming book</a>. This one seems a bit more friendlier (the Orange book is also very friendly) and goes a bit more into the details of the CUDA API while the Orange book focuses itself only the parts of the CUDA API that is needed to write the kernels.</p>
<h2 id="next-steps">Next steps</h2>
<p>These are just the initial steps into CUDA/GPU Programming. I was toying with the idea of buying a Jetson Orin or a Nano or a NVIDIA GPU enabled laptop, but seeing my storage box littered with a RPi 2, a RPi 4, A Vicharak Vaaman, I realized that I just love collecting toys and not really playing with them. Once I solve atleast 50% of the problems on Tensara, I believe it would be sign of me having committed sufficiently enough to justify such a purchase (or rent a GPU VM from vast.ai to run larger problems)</p>
<h1 id="paper-readup--fire-flyer-hpc">Paper readup : Fire-Flyer HPC</h1>
<p>DeepSeek wrote a paper on their <a href="https://arxiv.org/pdf/2408.14158">Fire-Flyer HPC Architecture for their GPU training systems</a>. DeepSeek v3 caught the world in a storm with their SOTA benchmarks and more importantly the ability of the Chinese in squeezing every bit of performance out of their weakened hardware that they are forced to buy due to Sanctions. They wrote a custom AllReduce implementation: HFReduce that performed better than Nvidia’s NCCL communication libraries to reduce bandwidth usage in Multi-GPU setups. China is on-part with the Frontier labs in the USA in terms of ability and the paper illustrates just how capable they have become in designing software and HPC architectures. DeepSeek V3 “aha” moment, was not the result of the Chinese stealing American trade secrets but serious capabilities built over decades of research and development in HPC.</p>  </div> </article> </main> <footer data-astro-cid-sz7xmlte> <div class="social-links" data-astro-cid-sz7xmlte> <a href="https://x.com/DeepknowledgeU" target="_blank" data-astro-cid-sz7xmlte> <span class="sr-only" data-astro-cid-sz7xmlte>Follow Astro on X</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" astro-icon="social/twitter" data-astro-cid-sz7xmlte><path fill="currentColor" d="M5.026 15c6.038 0 9.341-5.003 9.341-9.334 0-.14 0-.282-.006-.422A6.685 6.685 0 0 0 16 3.542a6.658 6.658 0 0 1-1.889.518 3.301 3.301 0 0 0 1.447-1.817 6.533 6.533 0 0 1-2.087.793A3.286 3.286 0 0 0 7.875 6.03a9.325 9.325 0 0 1-6.767-3.429 3.289 3.289 0 0 0 1.018 4.382A3.323 3.323 0 0 1 .64 6.575v.045a3.288 3.288 0 0 0 2.632 3.218 3.203 3.203 0 0 1-.865.115 3.23 3.23 0 0 1-.614-.057 3.283 3.283 0 0 0 3.067 2.277A6.588 6.588 0 0 1 .78 13.58a6.32 6.32 0 0 1-.78-.045A9.344 9.344 0 0 0 5.026 15z" data-astro-cid-sz7xmlte></path></svg> </a> <a href="https://github.com/Gowind" target="_blank" data-astro-cid-sz7xmlte> <span class="sr-only" data-astro-cid-sz7xmlte>My Github</span> <svg viewBox="0 0 16 16" aria-hidden="true" width="32" height="32" astro-icon="social/github" data-astro-cid-sz7xmlte><path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z" data-astro-cid-sz7xmlte></path></svg> </a> </div> </footer>  </body></html>